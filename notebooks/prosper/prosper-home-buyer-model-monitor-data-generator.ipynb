{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import session\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session\n",
    "sm_session = session.Session(boto3.Session())\n",
    "\n",
    "# S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# S3 bucket\n",
    "bucket = 'sagemaker-demo-third-party-models'\n",
    "\n",
    "# Model provider name\n",
    "prefix = 'prosper'\n",
    "\n",
    "# Model endpoint name\n",
    "endpoint_name = 'third-party-model-endpoint'\n",
    "\n",
    "# Data files\n",
    "buyer_zip_code_features_data_file = 'data/sample_basic_zip.csv'\n",
    "zip_code_features_data_file = 'data/zip_features.csv'\n",
    "#training_data_file = 'train/train-no-headers.csv'\n",
    "training_data_file = 'train/train.csv'\n",
    "\n",
    "# Create predictor endpoint\n",
    "predictor = Predictor(endpoint_name=endpoint_name, \n",
    "                      sagemaker_session=None, \n",
    "                      serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data dictionary\n",
    "training_data = {\n",
    "    'basic_zip': {'file_name' : buyer_zip_code_features_data_file, 'file_uri':'', 'data':''},\n",
    "    'zip_features': {'file_name' : zip_code_features_data_file, 'file_uri':'', 'data':''}}\n",
    "\n",
    "# S3 data file locations\n",
    "training_data['basic_zip']['file_uri'] = 's3://{}/{}/{}'.format(bucket, prefix, training_data['basic_zip']['file_name'])\n",
    "training_data['zip_features']['file_uri'] = 's3://{}/{}/{}'.format(bucket, prefix, training_data['zip_features']['file_name'])\n",
    "\n",
    "# Download files\n",
    "training_data['basic_zip']['data'] = S3Downloader.read_file(training_data['basic_zip']['file_uri'])\n",
    "training_data['zip_features']['data'] = S3Downloader.read_file(training_data['zip_features']['file_uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# print(training_data['basic_zip']['file_uri'])\n",
    "# print(training_data['zip_features']['file_uri'])\n",
    "\n",
    "# len(training_data['basic_zip']['data'].splitlines()) # 1000 rows\n",
    "# len(training_data['zip_features']['data'].splitlines()) # 28845 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_code_features(zip_code, zip_features_df):\n",
    "\n",
    "    # Get the zip code features using a Dataframe query\n",
    "    zip_features = zip_features_df[(zip_features_df.zip_code == zip_code)]\n",
    "       \n",
    "    # If no match found, then create empty encoding list\n",
    "    result = np.zeros((np.add(num_cluster_classes, num_division_classes)), dtype=int)    \n",
    "\n",
    "    # Defensive coding\n",
    "    if(len(zip_features) > 0):\n",
    "\n",
    "        # Get matching feature values\n",
    "        cluster = zip_features['cluster'].values[0]\n",
    "        division = zip_features['division'].values[0]\n",
    "\n",
    "        # One-hot encode feature values\n",
    "        cluster_encoded = np.eye(num_cluster_classes, dtype=int)[cluster]\n",
    "        division_encoded = np.eye(num_division_classes, dtype=int)[division]\n",
    "\n",
    "        # Concatenate the encoded features\n",
    "        result = np.concatenate( (cluster_encoded, division_encoded) )\n",
    "       \n",
    "    # Convert array of integers to a comma-delimited string\n",
    "    #result = \",\".join(result.astype(str))    \n",
    "    \n",
    "    # Return            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample, predictor=predictor, delay=0.5):\n",
    "   \n",
    "    # Defensive coding\n",
    "    if(len(sample) > 0):\n",
    "\n",
    "        print(sample)\n",
    "        \n",
    "        # Invoke the model's inference endpoint\n",
    "        response = predictor.predict(sample).decode('utf-8')\n",
    "        \n",
    "        # Suspends execution for # milliseconds\n",
    "        time.sleep(delay)        \n",
    "\n",
    "        # Return \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create baseline training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total encoded zip classes\n",
    "num_encoded_zip_classes = 25\n",
    "\n",
    "# Zipcode cluster classes\n",
    "num_cluster_classes = 16\n",
    "\n",
    "# Zipcode division classes\n",
    "num_division_classes = 9\n",
    "\n",
    "# Labels for data files\n",
    "cols = ['gender', 'age_range', 'household_income_range', 'zip_features']\n",
    "encoded_zip_feature_cols = np.array(['zip_feature_{}'.format(i) for i in range(0, num_encoded_zip_classes)])\n",
    "\n",
    "# Convert data files to dataframe\n",
    "basic_zip_df = pd.read_csv(StringIO(training_data['basic_zip']['data']), names=cols)\n",
    "\n",
    "zip_features_df = pd.read_csv(StringIO(training_data['zip_features']['data'])) # File includes col headers\n",
    "\n",
    "# Get list of one-hot encode zip features based on zipcode\n",
    "encoded_zip_features = [get_zip_code_features(zipcode, zip_features_df) for zipcode in basic_zip_df['zip_features']]\n",
    "\n",
    "# Update zip_features (zipcode) column with binary encoded zip features\n",
    "# basic_zip_df['zip_features'] = encoded_zip_features\n",
    "\n",
    "# Create dataframe for the encoded zip_features\n",
    "encoded_zip_features_df = pd.DataFrame(encoded_zip_features, columns=encoded_zip_feature_cols)\n",
    "\n",
    "# Concatenate the first three columns of the home buyers file with the encoded zip features\n",
    "df = pd.concat([basic_zip_df[['gender', 'age_range', 'household_income_range']], encoded_zip_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into memory (since it is a relatively small dataset) \n",
    "data_stream = StringIO()\n",
    "df.to_csv(data_stream, sep=',', encoding='utf-8', index=False) #header=False\n",
    "\n",
    "# Get stream data from memory\n",
    "train_csv = data_stream.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 bucket\n",
    "key = '{}/{}'.format(prefix, training_data_file)\n",
    "s3_client.put_object(Body=train_csv,\n",
    "                     Bucket=bucket, \n",
    "                     Key=key, \n",
    "                     ContentType='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create baseline training dataset with ground truth (optional)\n",
    "##### Be sure to check hourly rates before running a model's Batch Transform job\n",
    "\n",
    "1. Amazon SageMaker > Batch transform jobs > Create batch transform job\n",
    "2. Input data configuration: Split type = Line || Content type = text/csv\n",
    "3. Output data configuration: Assemble with = Line || Accept = text/csv\n",
    "4. Input/output filtering and data joins: Join source = Input - merge input data with job output\n",
    "5. Download output file from S3, rename to .csv, open file\n",
    "6. Add a header row, copy/paste the headers from training_data_file\n",
    "7. Insert a new first column\n",
    "8. Shift/move last column to new first column and name as 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert buyer zip_code samples to list\n",
    "samples = df.values.tolist()\n",
    "\n",
    "# Convert zip_code samples to list\n",
    "zip_samples = zip_features_df['zip_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate baseline data to trigger 'No Issues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay the training dataset as sample inference data\n",
    "batch_size = 10\n",
    "\n",
    "# Shuffle the samples\n",
    "random.shuffle(samples)\n",
    "\n",
    "# Invoke real-time inference endpoint using baseline data\n",
    "for index, sample in enumerate(samples[0:batch_size]):\n",
    "\n",
    "    # Removes the open/close bracket from string -- not required\n",
    "    sample = str(sample)[1:-1] \n",
    "    \n",
    "    # Get inference response\n",
    "    response = predict(sample)\n",
    "    \n",
    "    # Display the model's prediction probability\n",
    "    print('Sample {0} >> Input: {1}: >> Prediction: {2}'.format(index, sample, response))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data to induce data quality constraint violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# Set min/max ranges for 'gender', 'age_range', 'household_income_range'\n",
    "gender_min, gender_max = 0, 1\n",
    "age_range_min, age_range_max = 1, 6\n",
    "household_income_range_min, household_income_range_max = 0, 24\n",
    "\n",
    "# Set the noise_factor to some value that generates abnormal samples\n",
    "noise_factor = 1\n",
    "\n",
    "# Invoke real-time inference endpoint using baseline data\n",
    "for index in range(batch_size):\n",
    "                \n",
    "    # Assign random values to each feature\n",
    "    gender = random.randint(gender_min, gender_max) * noise_factor\n",
    "    age_range = random.randint(gender_min, gender_max) * noise_factor\n",
    "    household_income_range = random.randint(household_income_range_min, household_income_range_max) * noise_factor\n",
    "\n",
    "    # Shuffle the zip code samples\n",
    "    random.shuffle(zip_samples)\n",
    "\n",
    "    # Get random zipcode value\n",
    "    zip_features = get_zip_code_features(zip_samples[0], zip_features_df) * noise_factor\n",
    "    zip_features = \",\".join(zip_features.astype(str))    \n",
    "\n",
    "    # format request data as comma-delimited string\n",
    "    sample = f'{gender},{age_range},{household_income_range},{zip_features}'\n",
    "\n",
    "    # Get inference response\n",
    "    response = predict(sample)\n",
    "    \n",
    "    # Display the model's prediction probability\n",
    "    print('Sample {0} >> Input: {1}: >> Prediction: {2}'.format(index, sample, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring Schedule management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws sagemaker describe-monitoring-schedule --monitoring-schedule-name 'third-party-model-data-quality-schedule'\n",
    "# !aws sagemaker list-monitoring-executions --monitoring-schedule-name 'third-party-model-data-quality-schedule'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.\n",
    "# !aws sagemaker stop-monitoring-schedule --monitoring-schedule-name 'third-party-model-data-quality-schedule'\n",
    "# time.sleep(30) # allow time for processing\n",
    "# !aws sagemaker list-monitoring-schedules --endpoint-name 'third-party-model-endpoint'\n",
    "\n",
    "# Step 2.\n",
    "# !aws sagemaker delete-monitoring-schedule --monitoring-schedule-name 'third-party-model-data-quality-schedule'\n",
    "# time.sleep(30) # allow time for processing\n",
    "# !aws sagemaker list-monitoring-schedules --endpoint-name 'third-party-model-endpoint'\n",
    "\n",
    "# Step 3.\n",
    "# !aws sagemaker delete-endpoint --endpoint-name 'third-party-model-endpoint'\n",
    "# time.sleep(30) # allow time for processing\n",
    "# !aws sagemaker list-endpoints --name-contains 'third-party-model-endpoint'\n",
    "\n",
    "# Step 4.\n",
    "# !aws sagemaker delete-endpoint-config --endpoint-config-name 'third-party-model-endpoint-config'\n",
    "# time.sleep(30) # allow time for processing\n",
    "# !aws sagemaker list-endpoint-configs --name-contains 'third-party-model-endpoint-config'\n",
    "\n",
    "# Step 5.\n",
    "# !aws sagemaker delete-model --model-name 'third-party-model'\n",
    "# time.sleep(30) # allow time for processing\n",
    "# !aws sagemaker list-models --name-contains 'third-party-model'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
